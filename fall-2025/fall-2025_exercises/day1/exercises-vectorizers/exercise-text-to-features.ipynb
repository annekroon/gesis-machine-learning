{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Working with textual data\n",
    "\n",
    "### 0. Get the data.\n",
    "\n",
    "- Download the dataset from https://surfdrive.surf.nl/files/index.php/s/bfNFkuUVoVtiyuk. This is a subset of the data from https://doi.org/10.7910/DVN/YHWTFC. \n",
    "\n",
    "- Unpack it. On Linux and MacOS, you can do this with `tar -xzf mydata.tar.gz` on the command line. On Windows, you may need an additional tool such as `7zip` for that (note that technically speaking, there is a `tar` archive within a `gz` archive, so unpacking may take *two* steps depending on your tool).\n",
    "\n",
    "\n",
    "### 1. Inspect the structure of the dataset.\n",
    "What information do the following elements give you?\n",
    "\n",
    "- folder (directory) names\n",
    "- folder structure/hierarchy\n",
    "- file names\n",
    "- file contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Discuss strategies for working with this dataset!\n",
    "\n",
    "- Which questions could you answer?\n",
    "- How could you deal with it, given the size and the structure?\n",
    "- How much memory<sup>1</sup> (RAM) does your computer have? How large is the complete dataset? What does that mean?\n",
    "- Make a sketch (e.g., with pen&paper), how you could handle your workflow and your data to answer your question.\n",
    "\n",
    "<sup>1</sup> *memory* (RAM), not *storage* (harddisk)!\n",
    "\n",
    "### 3. Read some (or all?) data\n",
    "\n",
    "Here is some example code that you can modify. Assuming that the folder `articles` is in the same folder as the notebook you are currently working on, you could, for instance, do the following to read a *part* of your dataset.\n",
    "\n",
    "```python\n",
    "from glob import glob\n",
    "infowarsfiles = glob('articles/*/Infowars/*')\n",
    "infowarsarticles = []\n",
    "for filename in infowarsfiles:\n",
    "    with open(filename) as f:\n",
    "\t    infowarsarticles.append(f.read())\n",
    "\n",
    "```\n",
    "\n",
    "- Can you explain what the `glob` function does?\n",
    "- What does `infowarsfiles` contain, and what does `infowarsarticles` contain? First make an educated guess based on the code snippet, then check it! Do *not* print the whole thing, but use `len`, `type` en slicing `[:10]` to get the info you need.\n",
    "\n",
    "- Tip: take a random sample of the articles for practice purposes (if your code works, you can scale up!)\n",
    "\n",
    "```\n",
    "# taking a random sample of the articles for practice purposes\n",
    "articles =random.sample(infowarsarticles, 10)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os  \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "data_dir = r\"C:/Data Management/Gesis IML/articles-small\" #adjust this to your data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Vectorize the data\n",
    "\n",
    "Imagine you want to train a classifier that will predict whether articles come from a fake news source (e.g., `Infowars`) or a quality news outlet (e.g., `bbc`). In other words, you want to predict `source` based on linguistic variations in the articles.\n",
    "\n",
    "To arrive at a model that will do just that, you have to transform 'text' to 'features'.\n",
    "\n",
    "- Can you vectorize the data? Try defining different vectorizers. Consider the following options:\n",
    "    - `count` vs. `tfidf` vectorizers\n",
    "    - with/ without pruning\n",
    "    - with/ without stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fit a classifier\n",
    "\n",
    "- Try out a simple supervised model. Find some inspiration [here](possible-solution-exercise-day1.md). Can you predict the `source` using linguistic variations in the articles?\n",
    "\n",
    "- Which combination of pre-processing steps + vectorizer gives the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Inceasing efficiency + reusability\n",
    "The approach under (3) gets you very far.\n",
    "But for those of you who want to go the extra mile, here are some suggestions for further improvements in handling such a large dataset, consisting of thousands of files, and for deeper thinking about data handling:\n",
    "\n",
    "- Consider writing a function to read the data. Let your function take three parameters as input, `basepath` (where is the folder with articles located?), `month` and `outlet`, and return the articles that match this criterion.\n",
    "- Even better, make it a *generator* that yields the articles instead of returning a whole list.\n",
    "- Consider yielding a dict (with date, outlet, and the article itself) instead of yielding only the article text.\n",
    "- Think of the most memory-efficient way to get an overview of how often a given regular expression R is mentioned per outlet!\n",
    "- Under which circumstances would you consider having your function for reading the data return a pandas dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesis_iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
