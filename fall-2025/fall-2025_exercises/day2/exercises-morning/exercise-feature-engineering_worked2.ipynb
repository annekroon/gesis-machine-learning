{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: NLP and feature engineering\n",
    "----\n",
    "\n",
    "In this exercise, you can use one of yesterday's datasets (IMDB or the newspaper data). \n",
    "\n",
    "Today, we will use this data for analysis and feature extraction using NLP. \n",
    "\n",
    "These are important components of feature engineering: moving from textual data to a feature set that can be used in a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (0.16.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (78.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\stolw010\\appdata\\local\\anaconda3\\envs\\gesis_iml\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 2.4/12.8 MB 12.2 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 13.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.8 MB 14.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.3/12.8 MB 14.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 13.8 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # potentially you need to install the language model first, notice that as of spaCy v3.0 you use its full name instead of just 'en'  \n",
    "\n",
    "\n",
    "data_dir = r\"C:/Data Management/Gesis IML/articles-small\" #adjust this to your data directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in the data\n",
    "\n",
    "You can use the code you've written yesterday as a starting point. Again, try your code on a small sample of the data, and scale up later--once your confident that your code works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "infowarsfiles = glob(os.path.join(data_dir, 'articles/*/Infowars/*'))\n",
    "infowarsarticles = []\n",
    "for filename in infowarsfiles:\n",
    "    with open(filename) as f:\n",
    "\t    infowarsarticles.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. first analyses and pre-processing steps\n",
    "\n",
    "- Perform some first analyses on the data using string methods and regular expressions.\n",
    "Techniques you can try out include:\n",
    "\n",
    "a.  lowercasing  \n",
    "b.  tokenization  \n",
    "c.  stopword removal  \n",
    "d.  stemming and/or lemmatizing  \n",
    "e.  cleaning: removing punctuation, line breaks, double spaces  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a random sample of the articles for practice purposes\n",
    "articles = random.sample(infowarsarticles, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Charges', 'have', 'been', 'dropped', 'against', '11', 'members', 'of', 'Turkish', 'President', 'Recep', 'Tayyip', 'Erdogans', 'security', 'detail', 'that', 'were', 'accused', 'of', 'beating']\n"
     ]
    }
   ],
   "source": [
    "#get a list of words in this sample of articles:\n",
    "words = []\n",
    "for article in articles:\n",
    "    words.extend(article.split())\n",
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['charges', 'have', 'been', 'dropped', 'against', '11', 'members', 'of', 'turkish', 'president', 'recep', 'tayyip', 'erdogans', 'security', 'detail', 'that', 'were', 'accused', 'of', 'beating']\n"
     ]
    }
   ],
   "source": [
    "#now make sure all are lower case:\n",
    "words_lower = [word.lower() for word in words]\n",
    "print(words_lower[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: first 10 tokens of first article: ['recep', 'tayyip', 'erdogans', 'security', 'detail', 'that', 'were', 'accused', 'of', 'beating', 'protesters', 'in', 'washington', ',', 'd', '.', 'c', '.', 'federal', 'prosecutors']\n",
      "example: tokens of first article: [['charges', 'have', 'been', 'dropped', 'against', '11', 'members', 'of', 'turkish', 'president', 'recep', 'tayyip', 'erdogans', 'security', 'detail', 'that', 'were', 'accused', 'of', 'beating', 'protesters', 'in', 'washington', ',', 'd', '.', 'c', '.', 'federal', 'prosecutors', 'made', 'the', 'decision', 'to', 'drop', 'the', 'charges', 'against', '11', 'of', 'out', 'the', '15', 'security', 'members', 'in', 'connection', 'with', 'the', 'incident', '.', 'police', 'originally', 'announced', 'charges', 'against', '16', 'people', 'in', 'connection', 'with', 'the', 'violent', 'clashes', 'in', 'june', '.', 'the', 'scuffle', 'took', 'place', 'last', 'may', 'after', 'roughly', 'two', 'dozen', 'protesters', 'gathered', 'outside', 'of', 'the', 'turkish', 'embassy', 'to', 'protest', 'erdogans', 'policies', 'during', 'his', 'visit', 'to', 'washington', '.']]\n"
     ]
    }
   ],
   "source": [
    "#tokenize the words:\n",
    "# \\w+ matches a word,\n",
    "#'\\w+ matches a contraction (like 't, 's, etc.),\n",
    "#[^\\w\\s] matches any single character that is not a word character or whitespace.\n",
    "# The | operator tells the regex engine to match any one of these alternatives at each position in the text.\n",
    "\n",
    "def tokenize(text):\n",
    "    # This regex splits words and contractions into separate tokens, and handles punctuation\n",
    "    pattern = r\"\\w+|\\'\\w+|[^\\w\\s]\"\n",
    "    return re.findall(pattern, text.lower())\n",
    "\n",
    "# Example usage on the first article:\n",
    "tokens = tokenize(articles[0])\n",
    "print(\"example: first 20 tokens of first article that show the change:\", tokens[10:30]) #Note this range might need to be different for you depending on your random selection\n",
    "\n",
    "tokenized_words = [tokenize(article) for article in articles]\n",
    "print(\"example: tokens of first article:\", tokenized_words[:1]) # gives all tokens for first article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: tokens of first article: [['charges', 'dropped', '11', 'members', 'turkish', 'president', 'recep', 'tayyip', 'erdogans', 'security', 'detail', 'accused', 'beating', 'protesters', 'washington', ',', 'd', '.', 'c', '.', 'federal', 'prosecutors', 'made', 'decision', 'drop', 'charges', '11', '15', 'security', 'members', 'connection', 'incident', '.', 'police', 'originally', 'announced', 'charges', '16', 'people', 'connection', 'violent', 'clashes', 'june', '.', 'scuffle', 'took', 'place', 'last', 'may', 'roughly', 'two', 'dozen', 'protesters', 'gathered', 'outside', 'turkish', 'embassy', 'protest', 'erdogans', 'policies', 'visit', 'washington', '.']]\n"
     ]
    }
   ],
   "source": [
    "#stopword removal (list created for demonstration by GPT4.1)\n",
    "stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
    "    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\",\n",
    "    \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\",\n",
    "    \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\",\n",
    "    \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\",\n",
    "    \"now\"\n",
    "])\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "\n",
    "filtered_tokenized_words = [remove_stopwords(article) for article in tokenized_words]\n",
    "print(\"example: tokens of first article:\", filtered_tokenized_words[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: stemmed tokens of first article: [['charg', 'drop', '11', 'member', 'turkish', 'presid', 'recep', 'tayyip', 'erdogan', 'secur', 'detail', 'accus', 'beat', 'protest', 'washington', ',', 'd', '.', 'c', '.', 'feder', 'prosecutor', 'made', 'decis', 'drop', 'charg', '11', '15', 'secur', 'member', 'connect', 'incid', '.', 'polic', 'origin', 'announc', 'charg', '16', 'peopl', 'connect', 'violent', 'clash', 'june', '.', 'scuffl', 'took', 'place', 'last', 'may', 'roughli', 'two', 'dozen', 'protest', 'gather', 'outsid', 'turkish', 'embassi', 'protest', 'erdogan', 'polici', 'visit', 'washington', '.']]\n"
     ]
    }
   ],
   "source": [
    "#stem the tokens\n",
    "stemmer = PorterStemmer()\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "stemmed_tokenized_words = [stem_tokens(article) for article in filtered_tokenized_words]\n",
    "print(\"example: stemmed tokens of first article:\", stemmed_tokenized_words[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['charges dropped 11 members turkish president recep tayyip erdogans security detail accused beating protesters washington , d . c . federal prosecutors made decision drop charges 11 15 security members connection incident . police originally announced charges 16 people connection violent clashes june . scuffle took place last may roughly two dozen protesters gathered outside turkish embassy protest erdogans policies visit washington .']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(article) for article in filtered_tokenized_words][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example string: ['charges dropped 11 members turkish president recep tayyip erdogans security detail accused beating protesters washington , d . c . federal prosecutors made decision drop charges 11 15 security members connection incident . police originally announced charges 16 people connection violent clashes june . scuffle took place last may roughly two dozen protesters gathered outside turkish embassy protest erdogans policies visit washington .']\n",
      "example: lemmatized tokens of first article: [['charge', 'drop', '11', 'member', 'turkish', 'president', 'recep', 'tayyip', 'erdogan', 'security', 'detail', 'accuse', 'beat', 'protester', 'washington', ',', 'd', '.', 'c', '.', 'federal', 'prosecutor', 'make', 'decision', 'drop', 'charge', '11', '15', 'security', 'member', 'connection', 'incident', '.', 'police', 'originally', 'announce', 'charge', '16', 'people', 'connection', 'violent', 'clash', 'june', '.', 'scuffle', 'take', 'place', 'last', 'may', 'roughly', 'two', 'dozen', 'protester', 'gather', 'outside', 'turkish', 'embassy', 'protest', 'erdogan', 'policy', 'visit', 'washington', '.']]\n"
     ]
    }
   ],
   "source": [
    "#use the code from the slides for lemmatization:\n",
    "#first need to join tokens into a string for spacy to process it:\n",
    "print(\"example string:\", [\" \".join(article) for article in filtered_tokenized_words][:1])\n",
    "\n",
    "#use this input to lemmatize using spacy:\n",
    "lemmatized_tokens = [[token.lemma_ for token in nlp(\" \".join(article))] for article in filtered_tokenized_words]\n",
    "print(\"example: lemmatized tokens of first article:\", lemmatized_tokens[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: lemmatized tokens of first article after cleaning: [['charge', 'drop', '11', 'member', 'turkish', 'president', 'recep', 'tayyip', 'erdogan', 'security', 'detail', 'accuse', 'beat', 'protester', 'washington', 'd', 'c', 'federal', 'prosecutor', 'make', 'decision', 'drop', 'charge', '11', '15', 'security', 'member', 'connection', 'incident', 'police', 'originally', 'announce', 'charge', '16', 'people', 'connection', 'violent', 'clash', 'june', 'scuffle', 'take', 'place', 'last', 'may', 'roughly', 'two', 'dozen', 'protester', 'gather', 'outside', 'turkish', 'embassy', 'protest', 'erdogan', 'policy', 'visit', 'washington']]\n"
     ]
    }
   ],
   "source": [
    "#now define a regex to remove punctiation, linebreaks and spaces:\n",
    "pattern = r'[^\\w\\s]'  # matches any character that is not a word character or whitespace\n",
    "\n",
    "cleaned_tokens = []\n",
    "for article in lemmatized_tokens:\n",
    "    article_clean = [re.sub(pattern, '', token) for token in article]  # remove punctuation\n",
    "    article_clean = [token.strip() for token in article_clean if token.strip()]  # remove linebreaks and extra spaces\n",
    "    cleaned_tokens.append(article_clean)\n",
    "\n",
    "print(\"example: lemmatized tokens of first article after cleaning:\", cleaned_tokens[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. N-grams\n",
    "\n",
    "- Think about what type of n-grams you want to add to your feature set. Extract and inspect n-grams and/or collocations, and add them to your feature set if you think this is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract entities and other meaningful information\n",
    "\n",
    "Try to extract meaningful information from your texts. Depending on your interests and the nature of the data, you could:\n",
    "\n",
    "- use regular expressions to distinguish relevant from irrelevant texts, or to extract substrings\n",
    "- use NLP techniques such as Named Entity Recognition to extract entities that occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train a supervised classifier\n",
    "\n",
    "Go back to your code belonging to yesterday's assignment. Perform the same classification task, but this time carefully consider which feature set you want to use. Reflect on the options listed above, and extract features that you think are relevant to include. Carefully consider **pre-processing steps**: what type of features will you feed your algorithm? Do you, for example, want to manually remove stopwords, or include ngrams? Use these features as input for your classifier, and investigate the effects hereof on performance of the classifier. Not that the purpose is not to build the perfect classifier, but to inspect the effects of different feature engineering decisions on the outcomes of your classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS\n",
    "\n",
    "- Compare that bottom-up approach with a top-down (keyword or regular-expression based) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesis_iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
