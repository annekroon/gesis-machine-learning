{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: NLP and feature engineering\n",
    "----\n",
    "\n",
    "In this exercise, you can use one of yesterday's datasets (IMDB or the newspaper data). \n",
    "\n",
    "Today, we will use this data for analysis and feature extraction using NLP. \n",
    "\n",
    "These are important components of feature engineering: moving from textual data to a feature set that can be used in a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PACKAGES -\n",
    "#NOTE: you should import all packages you need here, but for now, we will do this in-line.\n",
    "\n",
    "\n",
    "#DATA - \n",
    "datadir = \"/Users/rupertkiddle/Desktop/teach/2024/Introduction to Machine Learning (GESIS)/3_datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in the data\n",
    "\n",
    "You can use the code you've written yesterday as a starting point. Again, try your code on a small sample of the data, and scale up later--once your confident that your code works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filepaths for the articles:\n",
    "from glob import glob #for filepaths\n",
    "infowarsfiles = glob(datadir+'/articles/*/Infowars/*')\n",
    "\n",
    "#initialize an empty list to store the articles:\n",
    "infowarsarticles = []\n",
    "\n",
    "#loop through the filepaths, open them, and append the articles to the list:\n",
    "for filename in infowarsfiles:\n",
    "    with open(filename) as f:\n",
    "        infowarsarticles.append(f.read())\n",
    "\n",
    "#how many articles do we have?\n",
    "len(infowarsarticles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's take a random sample of 10 articles:\n",
    "import random #for RNG\n",
    "articles = random.sample(infowarsarticles, 10)\n",
    "\n",
    "#just a sanity check:\n",
    "assert len(articles) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. first analyses and pre-processing steps\n",
    "\n",
    "- Perform some first analyses on the data using string methods and regular expressions.\n",
    "Techniques you can try out include:\n",
    "\n",
    "a.  lowercasing  \n",
    "b.  tokenization  \n",
    "c.  stopword removal  \n",
    "d.  stemming and/or lemmatizing  \n",
    "e.  cleaning: removing punctuation, line breaks, double spaces  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOWERCASING - \n",
    "\n",
    "#lowercase all the articles:\n",
    "articles_lower_cased = [art.lower() for art in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION, SIMPLE - \n",
    "\n",
    "#basic solution, using the string method `.split()`. \n",
    "articles_split = [art.split() for art in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION, ADVANCED -\n",
    "\n",
    "#more sophisticated solution, using the NLTK library.\n",
    "from nltk.tokenize import TreebankWordTokenizer #for tokenization\n",
    "articles_tokenized = [TreebankWordTokenizer().tokenize(art) for art in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION, MORE ADVANCED -\n",
    "import regex #for regular expressions\n",
    "import nltk #for natural language processing \n",
    "\n",
    "#even more sophisticated; create your own tokenizer that first split into sentences. In this way,`TreebankWordTokenizer` works better.\n",
    "nltk.download(\"punkt_tab\") \n",
    "\n",
    "class MyTokenizer:\n",
    "    def tokenize(self, text):\n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        result = []\n",
    "        word = r\"\\p{letter}\"\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            tokens = tokenizer.tokenize(sent)    \n",
    "            tokens = [t for t in tokens\n",
    "                      if regex.search(word, t)]\n",
    "            result += tokens\n",
    "        return result\n",
    "\n",
    "mytokenizer = MyTokenizer()\n",
    "\n",
    "print(mytokenizer.tokenize(articles[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORDS, SIMPLE - \n",
    "\n",
    "# define your stopwordlist:\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "mystopwords = stopwords.words(\"english\")\n",
    "mystopwords.extend([\"add\", \"more\", \"words\"]) # manually add more stopwords to your list if needed\n",
    "print(mystopwords) #let's see what's inside "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, remove stopwords from the corpus:\n",
    "articles_without_stopwords = []\n",
    "for article in articles:\n",
    "    articles_no_stop = \"\"\n",
    "    for word in article.lower().split():\n",
    "        if word not in mystopwords:\n",
    "            articles_no_stop = articles_no_stop + \" \" + word\n",
    "    articles_without_stopwords.append(articles_no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same solution, but with list comprehension:\n",
    "articles_without_stopwords = [\" \".join([w for w in article.lower().split() if w not in mystopwords]) for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOPWORDS, ADVANCED -\n",
    "\n",
    "# different--probably more sophisticated--solution, by writing a function and calling it in a list comprehension:\n",
    "def remove_stopwords(article, stopwordlist):\n",
    "    cleantokens = []\n",
    "    for word in article:\n",
    "        if word.lower() not in mystopwords:\n",
    "            cleantokens.append(word)\n",
    "    return cleantokens\n",
    "\n",
    "articles_without_stopwords = [remove_stopwords(art, mystopwords) for art in articles_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: it's good practice to frequently inspect the results of your code, to make sure you are not making mistakes, and the results make sense. \n",
    "# for example, compare your results to some random articles from the original sample:\n",
    "\n",
    "n = random.randint(0, 9)\n",
    "print(articles[n][:100])\n",
    "print(\"-----------------\")\n",
    "print(\" \".join(articles_without_stopwords[n])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEMMING AND LEMMATIZATION -\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer #for stemming\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stemmed_text = []\n",
    "for article in articles:\n",
    "    stemmed_words = \"\"\n",
    "    for word in article.lower().split():\n",
    "        stemmed_words = stemmed_words + \" \" + stemmer.stem(word)\n",
    "    stemmed_text.append(stemmed_words.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same solution, but with list comprehension:\n",
    "\n",
    "stemmed_text  = [\" \".join([stemmer.stem(w) for w in article.lower().split()]) for article in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare tokeninzation and lemmatization using `Spacy`:\n",
    "\n",
    "import spacy #for nlp\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#NOTE: you may need to spacy download en_core_web_sm\n",
    "lemmatized_articles = [[token.lemma_ for token in nlp(art)] for art in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, frequently inspect your code, and for example compare the results to the original articles:\n",
    "\n",
    "#pick a random article:\n",
    "n = random.randint(0, 9)\n",
    "\n",
    "print(articles[n][:100])\n",
    "print(\"-----------------\")\n",
    "print(stemmed_text[n][:100])\n",
    "print(\"-----------------\")\n",
    "print(\" \".join(lemmatized_articles[n])[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CLEANING: removing punctuation, line breaks, double spaces\n",
    "\n",
    "n = random.randint(0, 9)\n",
    "articles[n] # print a random article to inspect.\n",
    "\n",
    "## Typical cleaning up steps:\n",
    "from string import punctuation\n",
    "articles = [art.replace('\\n\\n', '') for art in articles] # remove line breaks\n",
    "articles = [\"\".join([w for w in art if w not in punctuation]) for art in articles] # remove punctuation\n",
    "articles = [\" \".join(art.split()) for art in articles] # remove double spaces by splitting the strings into words and joining these words again\n",
    "articles[n] # print the same article to see whether the changes are in line with what you want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. N-grams\n",
    "\n",
    "- Think about what type of n-grams you want to add to your feature set. Extract and inspect n-grams and/or collocations, and add them to your feature set if you think this is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_bigrams = [[\"_\".join(tup) for tup in nltk.ngrams(art.split(),2)] for art in articles] # creates bigrams\n",
    "articles_bigrams[7][:5] # inspect the results...\n",
    "\n",
    "# maybe we want both unigrams and bigrams in the feature set?\n",
    "assert len(articles)==len(articles_bigrams)\n",
    "\n",
    "articles_uniandbigrams = []\n",
    "for a,b in zip([art.split() for art in articles],articles_bigrams):\n",
    "    articles_uniandbigrams.append(a + b)\n",
    "\n",
    "#and let's inspect the outcomes again.\n",
    "articles_uniandbigrams[7]\n",
    "len(articles_uniandbigrams[7]),len(articles_bigrams[7]),len(articles[7].split())\n",
    "\n",
    "\n",
    "#Or, if you want to inspect collocations:\n",
    "text = [nltk.Text(tkn for tkn in art.split()) for art in articles ]\n",
    "text[7].collocations(num=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract entities and other meaningful information\n",
    "\n",
    "Try to extract meaningful information from your texts. Depending on your interests and the nature of the data, you could:\n",
    "\n",
    "- use regular expressions to distinguish relevant from irrelevant texts, or to extract substrings\n",
    "- use NLP techniques such as Named Entity Recognition to extract entities that occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and POS-tag with NLTK:\n",
    "tokens = [nltk.word_tokenize(sentence) for sentence in articles]\n",
    "tagged = [nltk.pos_tag(sentence) for sentence in tokens]\n",
    "print(tagged[0])\n",
    "\n",
    "\n",
    "#detect named entities with Spacy:\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = [nlp(sentence) for sentence in articles]\n",
    "for i in doc:\n",
    "    for ent in i.ents:\n",
    "        if ent.label_ == 'PERSON':\n",
    "            print(ent.text, ent.label_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: integrate these soltions - \n",
    "\n",
    "#removing stopwords:\n",
    "\n",
    "mystopwords = set(stopwords.words('english')) # use default NLTK stopword list; alternatively:\n",
    "#mystopwords = set(open('mystopwordfile.txt').readlines())  #read stopword list from a textfile with one stopword per line\n",
    "documents = [\" \".join([w for w in doc.split() if w not in mystopwords]) for doc in documents]\n",
    "documents[7]\n",
    "\n",
    "\n",
    "#using N-grams as features:\n",
    "documents_bigrams = [[\"_\".join(tup) for tup in nltk.ngrams(doc.split(),2)] for doc in documents] # creates bigrams\n",
    "documents_bigrams[7][:5] # inspect the results...\n",
    "\n",
    "#maybe we want both unigrams and bigrams in the feature set?\n",
    "assert len(documents)==len(documents_bigrams)\n",
    "\n",
    "documents_uniandbigrams = []\n",
    "for a,b in zip([doc.split() for doc in documents],documents_bigrams):\n",
    "    documents_uniandbigrams.append(a + b)\n",
    "\n",
    "#and let's inspect the outcomes again.\n",
    "documents_uniandbigrams[7]\n",
    "len(documents_uniandbigrams[7]),len(documents_bigrams[7]),len(documents[7].split())\n",
    "\n",
    "\n",
    "#or, if you want to inspect collocations:\n",
    "text = [nltk.Text(tkn for tkn in doc.split()) for doc in documents ]\n",
    "text[7].collocations(num=10)\n",
    "\n",
    "#NOTE: if you want to include n-grams as feature input, add the following argument to your vectorizer:*\n",
    "myvectorizer= CountVectorizer(analyzer=lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train a supervised classifier\n",
    "\n",
    "Go back to your code belonging to yesterday's assignment. Perform the same classification task, but this time carefully consider which feature set you want to use. Reflect on the options listed above, and extract features that you think are relevant to include. Carefully consider **pre-processing steps**: what type of features will you feed your algorithm? Do you, for example, want to manually remove stopwords, or include ngrams? Use these features as input for your classifier, and investigate the effects hereof on performance of the classifier. Not that the purpose is not to build the perfect classifier, but to inspect the effects of different feature engineering decisions on the outcomes of your classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using manually crafted features as input for supervised machine learning with `sklearn`\n",
    "import nltk # for NLP\n",
    "import random # for RNG\n",
    "from glob import glob # for filepaths\n",
    "from sklearn.model_selection import train_test_split # for creating train-test splits\n",
    "\n",
    "\n",
    "#define a function to read the data:\n",
    "def read_data(listofoutlets):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label in listofoutlets:\n",
    "        for file in glob(datadir+f'/articles/*/{label}/*'):\n",
    "            with open(file) as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(label)\n",
    "    return texts, labels\n",
    "\n",
    "#execute, returning corresponding lists of texts and labels:\n",
    "documents, labels = read_data(['Infowars', 'BBC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create bigrams and combine with unigrams  \n",
    "documents_bigrams = [[\"_\".join(tup) for tup in nltk.ngrams(doc.split(),2)] for doc in documents] # creates bigrams\n",
    "\n",
    "# maybe we want both unigrams and bigrams in the feature set?\n",
    "assert len(documents)==len(documents_bigrams)\n",
    "\n",
    "documents_uniandbigrams = []\n",
    "for a,b in zip([doc.split() for doc in documents],documents_bigrams):\n",
    "    documents_uniandbigrams.append(a + b)\n",
    "\n",
    "#and let's inspect the outcomes again.\n",
    "#documents_uniandbigrams[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some sanity checks:\n",
    "len(documents_uniandbigrams[7]),len(documents_bigrams[7]),len(documents[7].split())\n",
    "assert len(documents_uniandbigrams) == len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets fit a `sklearn` vectorizer on the manually crafted feature set:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X_train,X_test,y_train,y_test=train_test_split(documents_uniandbigrams, labels, test_size=0.3)\n",
    "\n",
    "#NOTE: we do *not* want scikit-learn to tokenize a string into a list of tokens,\n",
    "# after all, we already *have* a list of tokens. lambda x:x is just a fancy way of saying: do nothing!\n",
    "myvectorizer= CountVectorizer(analyzer=lambda x:x)\n",
    "\n",
    "\n",
    "\n",
    "#fit the vectorizer, and transform:\n",
    "X_features_train = myvectorizer.fit_transform(X_train)\n",
    "X_features_test = myvectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "#inspect the vocabulary and their id mappings\n",
    "\n",
    "# inspect\n",
    "myvectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally, run the model again\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_features_train, y_train)\n",
    "y_pred = model.predict(X_features_test)\n",
    "\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred)}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "###final remark on ngrams in scikit learn\n",
    "\n",
    "#Of course, you do not *have* to do all of this if you just want to use ngrams. Alternatively, you can simply use\n",
    "\n",
    "myvectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_features_train = myvectorizer.fit_transform(X_train)\n",
    "\n",
    "#*if X_train are the **untokenized** texts.*\n",
    "\n",
    "#what this little example illustrates, though, is that you can use *any* manually crafted feature set as input for scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS\n",
    "\n",
    "- Compare that bottom-up approach with a top-down (keyword or regular-expression based) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesis_iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
