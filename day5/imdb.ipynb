{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81be9717-52de-487c-98c4-77e4ac0b3f4a",
   "metadata": {},
   "source": [
    "# From Classic Machine Learning to Neural Networks and BERT: A brief tour with IMDB data\n",
    "\n",
    "This notebook is used to showcase different approaches as they are covered in the GESIS course \"An introduction to Supervised Machine Learning with Python\" by Anne Kroon and Damian Trilling.\n",
    "\n",
    "\n",
    "It is partly based on a tutorial by Orhan G. Yalçın published at https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a2edc1b-7857-4bf6-94de-958b403b3919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general-purpose libraries\n",
    "import os\n",
    "import bz2\n",
    "import urllib\n",
    "import tarfile\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# randomize order\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "# NB: Usually, you put all import statements here at the beginning of your script. \n",
    "# For didactic purposes, in this notebook, we will import the specific modules at the point when we introduce them instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726821e-1130-425f-8694-716bc5338240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8f8b498-904e-43f9-907e-b9d0f102957c",
   "metadata": {},
   "source": [
    "## Step 1: Get data\n",
    "\n",
    "We are going to work with the IMDB Movie dataset and predict whether movies are positive or negative. The following code just makes sure that you do not have to download it all over again -- if you have already downloaded it, it will just use the downlaoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3379b326-3e88-4a67-9926-2dd3be4925aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached file reviewdata.pickle.bz2\n"
     ]
    }
   ],
   "source": [
    "filename = \"reviewdata.pickle.bz2\"\n",
    "if os.path.exists(filename):\n",
    "    print(f\"Using cached file {filename}\")\n",
    "    with bz2.BZ2File(filename, \"r\") as zipfile:\n",
    "        data = pickle.load(zipfile)\n",
    "        text_train, text_test, y_train, y_test = data\n",
    "else:\n",
    "    url = \"https://cssbook.net/d/aclImdb_v1.tar.gz\"\n",
    "    print(f\"Downloading from {url}\")\n",
    "    fn, _headers = urllib.request.urlretrieve(url, filename=None)\n",
    "    t = tarfile.open(fn, mode=\"r:gz\")\n",
    "    text_train, text_test, y_train, y_test = [], [], [], []\n",
    "    for f in t.getmembers():\n",
    "        m=re.match(\"aclImdb/(\\w+)/(pos|neg)/\", f.name)\n",
    "        if not m:\n",
    "            continue  # skip folder names and unlabeled data\n",
    "        dataset, label = m.groups()\n",
    "        text = t.extractfile(f).read().decode(\"utf-8\")\n",
    "        if dataset == \"train\":\n",
    "            text_train.append(text)\n",
    "            y_train.append(label)\n",
    "        elif dataset == \"test\":\n",
    "            text_test.append(text)\n",
    "            y_test.append(label)\n",
    "    print(f\"Saving to {filename}\")\n",
    "    with bz2.BZ2File(filename, \"w\") as zipfile:\n",
    "        data = text_train, text_test, y_train, y_test\n",
    "        pickle.dump(data, zipfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe086db-de40-45dc-97f2-b312e1815299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25000 training and 25000 test samples.\n"
     ]
    }
   ],
   "source": [
    "assert len(text_train) == len(y_train)\n",
    "assert len(text_test) == len(y_test)\n",
    "\n",
    "print(f\"There are {len(y_train)} training and {len(y_test)} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9af054-6b99-4226-9f3c-6cff5a4d5b4b",
   "metadata": {},
   "source": [
    "### Randomize order\n",
    "In this specific case, the dataset is ordered: the first 12500 samples are negative and the last 12500 ones are positive. This can have unintended effects in training some models, and also makes it harder for us to just select, say, the last X samples as a validation dataset. We therefore just shuffle the data. Of course, we need to shuffle the texts and the labels *together* ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947a9598-bdb0-44fd-9035-df7c971d12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, y_train = shuffle(text_train, y_train, random_state=1983)\n",
    "text_test, y_test = shuffle(text_test, y_test, random_state=1983)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95ee4a4-4b8f-4479-81ec-e528d8e11e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to explore text_train, y_train, text_test, and y_test here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2020291-313d-4663-9f00-98d84e713532",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6122b924-9d5f-4a9f-ac43-b443299e3317",
   "metadata": {},
   "source": [
    "## Step 2: A baseline model\n",
    "\n",
    "To get a basic idea about what performance we can achieve, let's run a really basic baseline model: A Naïve Bayes classifier with a count vectorizer.\n",
    "\n",
    "\n",
    "**NB Note that we now already use the test dataset. If this was a serious research project, it would be advisable to instead set aside some test data to only use at the very end of this notebook to get a final estimate of the performance of the model we chose. To do so, you could split the test dataset here into a validation and a test dataset.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6224d47-9aeb-4acc-9ebc-02f16aa217ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880eacce-6bf8-4d35-a58a-0627683c67e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.78      0.88      0.82     12500\n",
      "         pos       0.86      0.75      0.80     12500\n",
      "\n",
      "    accuracy                           0.81     25000\n",
      "   macro avg       0.82      0.81      0.81     25000\n",
      "weighted avg       0.82      0.81      0.81     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "baseline.fit(text_train, y_train)\n",
    "y_pred = baseline.predict(text_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0a8bb2-1339-4083-85d8-c0c3cb3a918b",
   "metadata": {},
   "source": [
    "## Step 3: Some serious classical machine learning\n",
    "\n",
    "Let's see how far we can get with classical machine learning and do a grid search to try different vectorizer settings and different penalties for a Logistic Regression. Of course, we can also test a lot of other models like Random Forests, ADABoost, SGD, ... --- but as we see, this works really well:\n",
    "\n",
    "(Note that scoring on accuracy, as we do here, is a very bad idea with unbalanced classes - but in our case, they are perfectly balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ac2b3b-f6d9-4d88-b220-b5ca5bcf83ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf428ffd-5cbb-4c64-aa79-9ec1f3aefd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters: {'clf__C': 100, 'tfidf__use_idf': True, 'vect__max_df': 0.5, 'vect__ngram_range': (1, 2)}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.90      0.90      0.90     12500\n",
      "         pos       0.90      0.90      0.90     12500\n",
      "\n",
      "    accuracy                           0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(solver=\"liblinear\")),\n",
    "])\n",
    "\n",
    "grid = {\n",
    "    'vect__max_df': (0.5, 0.75),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),  # tfidf or not\n",
    "    'clf__C': (.01, 1, 100),\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(estimator=pipeline, param_grid=grid, cv=5,\n",
    "                      scoring=\"accuracy\", n_jobs=-1, verbose=3)\n",
    "search.fit(text_train, y_train)\n",
    "print(f\"Best parameters: {search.best_params_}\")\n",
    "pred = search.predict(text_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc2743-2ade-49d7-8764-d7a16a095939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc350b-398a-4416-895f-a813624a90b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7755be8-0fc0-40be-80f3-5afd181ca66e",
   "metadata": {},
   "source": [
    "## Step 4: Neural networks\n",
    "\n",
    "# TODO TOEVOEGEN NAV VOORBEELD https://github.com/damian0604/embeddingworkshop/blob/main/06downstreamkeras.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d354899-f3bc-44c5-8927-6f6ff8741992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fcd685-a6a6-412c-b2df-ecc725e2f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb0206e3-d93f-48de-ade2-3448c25d7c13",
   "metadata": {},
   "source": [
    "## Step 5: Transformers\n",
    "\n",
    "some text here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab44edff-150f-415f-8b06-93512c2872fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-16 17:30:07.302252: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-08-16 17:30:07.302281: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9f030b-eeb2-4d3e-ac23-b53e3415c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-16 17:30:09.727678: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-08-16 17:30:09.727711: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-08-16 17:30:09.727732: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aa892048da11): /proc/driver/nvidia/version does not exist\n",
      "2021-08-16 17:30:09.727984: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b85439ac-abcb-4095-9b48-08ce485f0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b616b2c-fd3c-4ccf-b91e-a0b62462c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_examples(texts, labels, mapping = {\"pos\":1, \"neg\":0}):\n",
    "    for text, label in zip(texts, labels):\n",
    "        label_encoded = mapping.get(label, label)\n",
    "        yield InputExample(guid=None, text_a=text, label=label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e2c6853-e79b-444c-8f07-8f90e618421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use 20% of the test set for validation\n",
    "\n",
    "VALIDATIONSIZE = int(.2 * len(y_test))\n",
    "\n",
    "train_examples = create_tf_examples(text_train, y_train)\n",
    "validation_examples = create_tf_examples(text_test[:VALIDATIONSIZE], y_test[:VALIDATIONSIZE])\n",
    "test_examples = create_tf_examples(text_test[VALIDATIONSIZE:], y_test[VALIDATIONSIZE:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbd2cb22-299c-46bc-8329-170f020528a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function taken and slightly adapted from https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671\n",
    "\n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = []\n",
    "    for e in examples:\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            padding=\"max_length\", \n",
    "            truncation=True)\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(InputFeatures(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label))\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield ({\"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,},\n",
    "                f.label,)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        ({\"input_ids\": tf.TensorShape([None]), \"attention_mask\": tf.TensorShape([None]), \"token_type_ids\": tf.TensorShape([None]),}, tf.TensorShape([]),),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7db113ad-7e3d-4d22-a5ed-65dc6801b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = convert_examples_to_tf_dataset(train_examples, tokenizer)\n",
    "validation_data = convert_examples_to_tf_dataset(validation_examples, tokenizer)\n",
    "test_data = convert_examples_to_tf_dataset(test_examples, tokenizer)\n",
    "\n",
    "train_data = train_data.shuffle(100).batch(32).repeat(2)\n",
    "validation_data = validation_data.batch(32)\n",
    "test_data = test_data.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c3ab5-ee8f-4aae-99da-b77a55b595f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8c006ec820>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f8c006ec820>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-16 17:36:07.964327: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    735/Unknown - 4426s 6s/step - loss: 0.3397 - accuracy: 0.8484"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "model.fit(train_data, epochs=2, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0d394-0e95-48e9-8b9d-33903e0e65ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0afc4-05b3-475d-9d7f-e4b3e612adb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
