{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: NLP and feature engineering\n",
    "----\n",
    "\n",
    "In this exercise, you can use one of yesterday's datasets (IMDB or the newspaper data). \n",
    "\n",
    "Today, we will use this data for analysis and feature extraction using NLP. \n",
    "\n",
    "These are important components of feature engineering: moving from textual data to a feature set that can be used in a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read in the data\n",
    "\n",
    "You can use the code you've written yesterday as a starting point. Again, try your code on a small sample of the data, and scale up later--once your confident that your code works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infowarsfiles = glob('articles/*/Infowars/*')\n",
    "infowarsarticles = []\n",
    "for filename in infowarsfiles:\n",
    "    with open(filename) as f:\n",
    "        infowarsarticles.append(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. first analyses and pre-processing steps\n",
    "\n",
    "- Perform some first analyses on the data using string methods and regular expressions.\n",
    "Techniques you can try out include:\n",
    "\n",
    "a.  lowercasing  \n",
    "b.  tokenization  \n",
    "c.  stopword removal  \n",
    "d.  stemming and/or lemmatizing  \n",
    "e.  cleaning: removing punctuation, line breaks, double spaces  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. N-grams\n",
    "\n",
    "- Think about what type of n-grams you want to add to your feature set. Extract and inspect n-grams and/or collocations, and add them to your feature set if you think this is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract entities and other meaningful information\n",
    "\n",
    "Try to extract meaningful information from your texts. Depending on your interests and the nature of the data, you could:\n",
    "\n",
    "- use regular expressions to distinguish relevant from irrelevant texts, or to extract substrings\n",
    "- use NLP techniques such as Named Entity Recognition to extract entities that occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train a supervised classifier\n",
    "\n",
    "Go back to your code belonging to yesterday's assignment. Perform the same classification task, but this time carefully consider which feature set you want to use. Reflect on the options listed above, and extract features that you think are relevant to include. Carefully consider **pre-processing steps**: what type of features will you feed your algorithm? Do you, for example, want to manually remove stopwords, or include ngrams? Use these features as input for your classifier, and investigate the effects hereof on performance of the classifier. Not that the purpose is not to build the perfect classifier, but to inspect the effects of different feature engineering decisions on the outcomes of your classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS\n",
    "\n",
    "- Compare that bottom-up approach with a top-down (keyword or regular-expression based) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesis_iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
