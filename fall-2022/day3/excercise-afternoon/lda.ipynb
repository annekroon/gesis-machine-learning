{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling with gensim\n",
    "\n",
    "1. Algorithms/formal background\n",
    "2. From text to features: preprocessing, tokens, n-grams \n",
    "3. Heuristics for the removal of stopwords \n",
    "4. Tuning of the topic number for optimal model fit \n",
    "5. Evaluating and comparing different models \n",
    "6. Visualizing and interactively exploring topic models\n",
    "7. What’s the next step in the pipeline? Using the results of a topic model\n",
    "8. Plotting overtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "We assume that you run Python 3 and have NLTK (Bird, Loper, & Klein, 2009) installed. If you use Anaconda, you have it anyway. Otherwise, use \n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "or \n",
    "```\n",
    "sudo pip install nltk\n",
    "```\n",
    "(or possibly pip3) in your terminal to install it.\n",
    "\n",
    "We also assume you have `gensim` and `pyldavis` installed, if not, do so as well using pip.\n",
    "\n",
    "Furthermore, we have to download some data for some specific NLTK modules. Download them by executing the following cell (you only have to do this once):\n",
    "\n",
    "Bird, S., Loper, E., & Klein, E. (2009). *Natural language processing with Python*. Sebastopol, CA: O'Reilly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started!\n",
    "\n",
    "## Import modules\n",
    "Before we start, let's import some modules that we need today. It is good practice to do so at the beginning of a script, so we'll do it right now and not later when we need them. The benefit is that you immediately see if something goes wrong (for instance, because the module is not installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from glob import glob\n",
    "from string import punctuation\n",
    "import random\n",
    "random.seed(\"ic2s2colgne\")\n",
    "from nltk.sentiment import vader\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "print(\"STARTING NOW:\",str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Your gensim version is {gensim.__version__}. This notebook assumes that you have version 4.0 or higher. If not, please upgrade.\")\n",
    "assert int(gensim.__version__[0])>=4\n",
    "\n",
    "print(f\"Your pyLDAvis version is {pyLDAvis.__version__}. This notebook assumes that you have version 3.0 or higher. If not, please upgrade.\")\n",
    "assert int(pyLDAvis.__version__[0])>=3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set a random seed *and* use `distributed=False` as an argument to your models, you get reprodable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1983)\n",
    "np.random.seed(1983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "We will use a dataset by Schumacher et al. (2016). From the abstract:\n",
    "> This paper presents EUSpeech, a new dataset of 18,403 speeches from EU leaders (i.e., heads of government in 10 member states, EU commissioners, party leaders in the European Parliament, and ECB and IMF leaders) from 2007 to 2015. These speeches vary in sentiment, topics and ideology, allowing for fine-grained, over-time comparison of representation in the EU. The member states we included are Czech Republic, France, Germany, Greece, Netherlands, Italy, Spain, United Kingdom, Poland and Portugal.\n",
    "\n",
    "Schumacher, G, Schoonvelde, M., Dahiya, T., Traber, D, & de Vries, E. (2016): *EUSpeech: a New Dataset of EU Elite Speeches*. [doi:10.7910/DVN/XPCVEI](http://dx.doi.org/10.7910/DVN/XPCVEI)\n",
    "\n",
    "Download and unpack the following file:\n",
    "```\n",
    "speeches_csv.tar.gz\n",
    "```\n",
    "\n",
    "In the .tar.gz file, you find a .zip file. Extract the whole folder to your home directory.\n",
    "See below a screenshot of how this looks like in Lubuntu (double-click on \"speeches_csv.zip\" in the left window, then the right window will open. Click on \"Extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"https://github.com/damian0604/bdaca/raw/master/ipynb/euspeech_download.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the files we downloaded.\n",
    "\n",
    "**NB: This command line magic only works on Linux and MacOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ~/Downloads/Cleaned_Speeches/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start!\n",
    "Let's retrieve a list of all speeches from one of the files. Of course, we could also loop over all the files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filelist = glob('Cleaned_Speeches/Speeches_*_Cleaned.csv')\n",
    "# instead of all speeches, to speed up things, we are focusing on speeches from the Netherlands only\n",
    "filelist = glob('speeches_UK_Cleaned.csv')\n",
    "print(filelist)\n",
    "speeches_eng=[]\n",
    "for fn in filelist:\n",
    "    with open(fn) as fi:\n",
    "        reader=csv.reader(fi)\n",
    "        for row in reader:\n",
    "            if row[7]=='en':   # only include english-language speches; we might as well choose 'nl' or 'fr'\n",
    "                speeches_eng.append(row[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speeches_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed things up, we could also sample some speeches. \n",
    "# speeches = random.sample(speeches,100)\n",
    "# len(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_eng[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. From text to features: preprocessing, tokens, n-grams \n",
    "## General approach\n",
    "\n",
    "From a machine-learning perspective, one could argue that all information in a text might be useful information. However, we are interested in getting *interpretable* topics, so even if for instance the use of specific HTML tags would help us distinguising between some documents, we want to get rid of them. More in general, we start by cleaning up a bit to get only 'real' text.\n",
    "\n",
    "### Some typical clean-up steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_eng=[speech.replace('<p>',' ').replace('</p>',' ') for speech in speeches_eng]   #remove HTML tags\n",
    "speeches_eng=[\"\".join([l for l in speech if l not in punctuation]) for speech in speeches_eng]  #remove punctuation\n",
    "speeches_eng=[speech.lower() for speech in speeches_eng]  # convert to lower case\n",
    "speeches_eng=[\" \".join(speech.split()) for speech in speeches_eng]   # remove double spaces by splitting the strings into words and joining these words again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first speech to check everything's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_eng[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as with other unsupervised machine learning techniques, we are not really interesting a long string of text. We rather want to have each document being represented by a set of *features*. To this end, `gensim` has a finciton `doc2bow` that converts a list of words (tokens) to `(token_id, token_count)` tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid messing things up, I'll call all objects relating to our first model _m1\n",
    "ldainput_m1 = [speech.split() for speech in speeches_eng]           # convert all strings to list of words\n",
    "id2word_m1 = corpora.Dictionary(ldainput_m1)                        # assign a token_id to each word\n",
    "ldacorpus_m1 = [id2word_m1.doc2bow(doc) for doc in ldainput_m1]     # represent each speech by (token_id, token_count) tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just run a simple LDA on this to check out whether it works. We specify the corpus (in wich each document is represented by a `(token_id, token_count)` tuple), the table to translate the token_id's back to words, and the number of topics we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_m1 = models.LdaModel(ldacorpus_m1, id2word=id2word_m1, num_topics=10)\n",
    "lda_m1.print_topics()\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "We immediately see that the result of our first LDA doesn't make much sense: We see only stopwords. \n",
    "\n",
    "\n",
    "### Explicit stopword removal\n",
    "The most straightforward approach is to use a pre-existing list with stopwords, possibly with the addition of some own, case-specific words. We then split up each speech in words, and only if a word is not on the stopwordlist, we keep it and join it with the previous and next word using a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords = set(stopwords.words('english')) # use default NLTK stopword list; alternatively:\n",
    "# mystopwords = set(open('mystopwordfile.txt').readlines())  #read stopword list from a textfile with one stopword per line\n",
    "\n",
    "speeches_eng_clean = [\" \".join([w for w in speech.split() if w not in mystopwords]) for speech in speeches_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_eng_clean[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether this looks better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldainput_m2 = [speech.split() for speech in speeches_eng_clean]      # speechesclean instead of speeches\n",
    "id2word_m2 = corpora.Dictionary(ldainput_m2)                       \n",
    "ldacorpus_m2 = [id2word_m2.doc2bow(doc) for doc in ldainput_m2]  \n",
    "lda_m2 = models.LdaModel(ldacorpus_m2, id2word=id2word_m2, num_topics=10)\n",
    "lda_m2.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF scores instead of word counts as features\n",
    "Explicitly removing stopwords is a common practice and often very useful. We shouldn't forget, though, that there are some problematic aspects to it as well\n",
    "- It is kind of arbitrary what is on the stopword list and what now\n",
    "- Depending on the research question one is interested in, it might differ what words are 'meaningful'\n",
    "- Although the list is meant to consist of words that occur with a high frequency in all texts, it is not based on actual frequencies in the corpus but set a priori.\n",
    "A different approach would therefore be to simply use (a) the frequency of each word in the corpus and (b) the number of documents in which the document occurs. \n",
    "In other words: If we use tf-idf scores (term frequency weighed by the inverse document frequncy) instead of raw word counts as featues, the stopwords should disappear automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldacorpus_m3 = ldacorpus_m1       # reuse corpus from Model 1 \n",
    "id2word_m3 = id2word_m1           # and thus, also use id2word-mapping\n",
    "tfidfcorpus_m3 = models.TfidfModel(ldacorpus_m3)\n",
    "lda_m3 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m3[ldacorpus_m3],id2word=id2word_m3,num_topics=10)\n",
    "lda_m3.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering extremes\n",
    "Just as we don't want to include words that are all over the place and do little to distinguish documents, we also do not want to include words that virtually never occur. If among millions of words, a word occurs exactly one time, it might be simply a spelling mistake. But even if it is not, it does not help us to infer topics across documents. \n",
    "\n",
    "Also in purely pragmatic terms, it makes sense to remove unneccessary features to speed up the analysis process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_m4 = corpora.Dictionary(ldainput_m1)           # reuse input from M1     \n",
    "\n",
    "id2word_m4.filter_extremes(no_below=5, no_above=0.5)   # do not consider all words that occur in less than n=5 documents\n",
    "                                                       # or in more than 50% of all documents.\n",
    "\n",
    "ldacorpus_m4 = [id2word_m4.doc2bow(doc) for doc in ldainput_m1]\n",
    "tfidfcorpus_m4 = models.TfidfModel(ldacorpus_m4)\n",
    "lda_m4 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m4[ldacorpus_m4],id2word=id2word_m4,num_topics=10, distributed=False, random_state=42) \n",
    "lda_m4.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other preprocessing ideas\n",
    "\n",
    "### Stemming\n",
    "Stemming can be useful to avoid that 'economics', 'economic', and 'economy' are seen as different concepts by the topic model. In practice, however, standard stemming algorithms are far from perfect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "speeches_eng_stemmed = [\" \".join([stemmer.stem(word) for word in speech.split()]) for speech in speeches_eng]\n",
    "speeches_eng_stemmed[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and retaining only nouns and adjectives\n",
    "Depending on the specific use case at hand, one might discover that some parts of speech (POS) are more informative than others. We could, for instance, create a topic model based on only the nouns and adjectives in a text, disregarding everything else. \n",
    "Look at the NLTK documentation to find out what each code means (e.g., 'NN' is 'noun') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_eng_nounsadj=[]\n",
    "for speech in speeches_eng:\n",
    "    tokens = nltk.word_tokenize(speech)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    cleanspeech = \"\"\n",
    "    for element in tagged:\n",
    "        if element[1] in ('NN','NNP','JJ'):\n",
    "            cleanspeech=cleanspeech+element[0]+\" \"\n",
    "    speeches_eng_nounsadj.append(cleanspeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_eng_nounsadj[0][:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using ngrams as features\n",
    "Topic models follow a bag-of-words approach, meaning they do not take word order into account. However, sometimes we want to be able to do so to a limited extend: The \"white house\" is something else than a \"house with a white wall\", even though both strings contain the words 'white' and 'house'. We can do so by joining adjacent words together in so-called bigrams (or trigrams, if we take three words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_eng_bigrams = [[\"_\".join(tup) for tup in nltk.ngrams(speech.split(),2)] for speech in speeches_eng_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_eng_bigrams[0][100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe we want both unigrams and bigrams in the feature set?\n",
    "assert len(speeches_eng_clean)==len(speeches_eng_bigrams)\n",
    "speeches_eng_uniandbigrams = []\n",
    "for a,b in zip([speech.split() for speech in speeches_eng_clean],speeches_eng_bigrams):\n",
    "    speeches_eng_uniandbigrams.append(a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(speeches_eng_uniandbigrams[6]),len(speeches_eng_bigrams[6]),len(speeches_eng_clean[6].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_m5 = corpora.Dictionary(speeches_eng_uniandbigrams)                       \n",
    "id2word_m5.filter_extremes(no_below=5, no_above=0.5)\n",
    "ldacorpus_m5 = [id2word_m5.doc2bow(doc) for doc in speeches_eng_uniandbigrams]\n",
    "tfidfcorpus_m5 = models.TfidfModel(ldacorpus_m5)\n",
    "lda_m5 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m5[ldacorpus_m5],id2word=id2word_m5,num_topics=10)\n",
    "lda_m5.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up\n",
    "While there are different ways to achieve the desired results and different modules to help you with it (writing from scratch, NLTK, but also gensim.utils), these are some steps to consider when transforming texts to feature sets for topic modeling (recall that not all of them might be neccessary of even diserable, depending on the use case):\n",
    "\n",
    "- transforming to lowercase\n",
    "- removing stopwords\n",
    "- stemming\n",
    "- POS-tagging (and removing unwanted elements)\n",
    "- filtering extremely common and extremely uncommon words\n",
    "- ngrams and/or unigrams as features?\n",
    "- raw frequencies or TF-IDF scores as features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluating and comparing different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence\n",
    "We can calculate the Umass topic coherence for each topic. See Mimno, Wallach, Talley, Leenders, McCallum: Optimizing Semantic Coherence in Topic Models, CEMNLP 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print coherence per topic:\n",
    "lda_m5.top_topics(ldacorpus_m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or: overall coherence\n",
    "# NB: CoherenceModel is only available in newer versions of gensim.\n",
    "# if it's not available, consider upgrading with `pip3 install gensim -upgrade`\n",
    "cm1 = models.CoherenceModel(model=lda_m1, corpus=ldacorpus_m1, dictionary= id2word_m1, coherence='u_mass')  \n",
    "naivecoh = cm1.get_coherence()\n",
    "cm2 = models.CoherenceModel(model=lda_m2, corpus=ldacorpus_m2, dictionary= id2word_m2, coherence='u_mass')  \n",
    "cleancoh = cm2.get_coherence()\n",
    "#cm3 = models.CoherenceModel(model=lda_m3, corpus=ldacorpus_m3, coherence='u_mass')\n",
    "cm3 = models.CoherenceModel(model=lda_m3, corpus=tfidfcorpus_m3[ldacorpus_m3], dictionary= id2word_m3, coherence='u_mass')\n",
    "tfidfcoh = cm3.get_coherence()\n",
    "cm4 = models.CoherenceModel(model=lda_m4, corpus=tfidfcorpus_m4[ldacorpus_m4], dictionary= id2word_m4, coherence='u_mass')\n",
    "tfidffiltercoh = cm4.get_coherence()\n",
    "print(\"Coherence of naive model = {}\\nCoherence of clean model = {}\\nCoherence of tf-idf model = {}\\nCoherence of tf-idf model without extreme words {}\".format(naivecoh, cleancoh, tfidfcoh,tfidffiltercoh))\n",
    "print(\"NB: Note that it may not make too much sense to compare these vaues across different corpora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_m4_bad = models.LdaModel(tfidfcorpus_m4[ldacorpus_m4], id2word=id2word_m4, num_topics=10,iterations=1)\n",
    "print(models.CoherenceModel(model=lda_m4_bad, corpus=tfidfcorpus_m4[ldacorpus_m4], coherence='u_mass').get_coherence())\n",
    "\n",
    "lda_m4_good = models.LdaModel(tfidfcorpus_m4[ldacorpus_m4], id2word=id2word_m4, num_topics=10,iterations=50, passes=5)\n",
    "print(models.CoherenceModel(model=lda_m4_good, corpus=tfidfcorpus_m4[ldacorpus_m4], coherence='u_mass').get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_m4_bad.top_topics(tfidfcorpus_m4[ldacorpus_m4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_m4_good.top_topics(tfidfcorpus_m4[ldacorpus_m4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_m3.top_topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= cm1.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visualizing and interactively exploring topic models\n",
    "A great tool for interactively exploring topicmodels is pyLDAvis.\n",
    "pyLDAvis can estimate its own topic models, but it als has a nice function called `gensim.prepare`, which you can use to visualize the model you already estimated with gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(lda_m5,ldacorpus_m5,id2word_m5)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. What’s the next step in the pipeline? Using the results of a topic model\n",
    "\n",
    "Until know, we have mainly considered the interpretation of the topics themselves. While it can indeed be interesting to use topic models to summarize and interpret large corpora, this is usually not where social scientists stop: We want to relate the topics back to documents to say something about which topics occur in which documents.\n",
    "\n",
    "## Saving topic scores to a file\n",
    "Somewhat similar to factor analysis and principal component analysis, where one can also store factor scores that indicate how high a specific case scores on each of the factors that were identified, for each document, we can estimate a score for each of the topics we identified.\n",
    "\n",
    "To do so, we can simply call the `.inference()` method on the model we estimated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresperdoc=lda_m5.inference(ldacorpus_m5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(scoresperdoc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do a lot of different stuff with the resulting matrix, in which each row represents one of the documents and each row consists of one score for each topic.\n",
    "For example, we just could create a tab-separated file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"topicscores.tsv\",\"w\",encoding=\"utf-8\") as fo:\n",
    "    for row in scoresperdoc[0]:\n",
    "       fo.write(\"\\t\".join([\"{:0.3f}\".format(score) for score in row]))\n",
    "       fo.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or we put it into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(scoresperdoc[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that these scores are extremely skewed. Maybe we just want to know which topics score really high? Let's recode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df.applymap(lambda x: int(x>10))\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a heatmap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 20))\n",
    "sns.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning alpha and eta\n",
    "different parameters. From docstring:\n",
    "```\n",
    "`alpha` and `eta` are hyperparameters that affect sparsity of the document-topic\n",
    "(theta) and topic-word (lambda) distributions. Both default to a symmetric\n",
    "1.0/num_topics prior.\n",
    "\n",
    "`alpha` can be set to an explicit array = prior of your choice. It also\n",
    "support special values of 'asymmetric' and 'auto': the former uses a fixed\n",
    "normalized asymmetric 1.0/topicno prior, the latter learns an asymmetric\n",
    "prior directly from your data.\n",
    "\n",
    "`eta` can be a scalar for a symmetric prior over topic/word\n",
    "distributions, or a matrix of shape num_topics x num_words, which can\n",
    "be used to impose asymmetric priors over the word distribution on a\n",
    "per-topic basis. This may be useful if you want to seed certain topics\n",
    "with particular words by boosting the priors for those words.  It also\n",
    "supports the special value 'auto', which learns an asymmetric prior\n",
    "directly from your data.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example for different specification: repeat analysis 10 times, while learning alpha and eta from the data \n",
    "# instead of using 1/number of topics as defailt\n",
    "lda_m6 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m5[ldacorpus_m5],id2word=id2word_m5,num_topics=50, alpha='auto', eta = 'auto',passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with this specific data, this fails b/c of a fitting issue (there is a complex number instead of a float returned somewhere)\n",
    "vis_data = gensimvis.prepare(lda_m6,ldacorpus_m5,id2word_m5)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Plotting over time\n",
    "\n",
    "Please find here another example that illustrates how to plot topic scores over time, as one example for follow-up analysis.\n",
    "\n",
    "\n",
    "The `.inference()` method used on the slides and above under chapter 7 gives raw gamma scores (between zero and +infinity). That’s a bit hard to interpret in a plot, so here we will use `.get_document_topics` instead. That will return normalized scores. You can achieve the same result by normalizing the output of `.inference()` yourself (e.g., `normalizedtopicscores = (originalscores.T/originalscores.sum(axis=1)).T`)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = glob('speeches_UK_Cleaned.csv')\n",
    "print(filelist)\n",
    "dates = []\n",
    "speeches_eng=[]\n",
    "for fn in filelist:\n",
    "    with open(fn) as fi:\n",
    "        reader=csv.reader(fi)\n",
    "        for row in reader:\n",
    "            if row[7]=='en':   # only include english-language speches; we might as well choose 'nl' or 'fr'\n",
    "                speeches_eng.append(row[5])\n",
    "                dates.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches=[speech.replace('<p>',' ').replace('</p>',' ') for speech in speeches_eng]   #remove HTML tags\n",
    "speeches=[\"\".join([l for l in speech if l not in punctuation]) for speech in speeches]  #remove punctuation\n",
    "speeches=[speech.lower() for speech in speeches]  # convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldainput_m10 = [speech.split() for speech in speeches]           \n",
    "id2word_m10 = corpora.Dictionary(ldainput_m10)                       \n",
    "\n",
    "id2word_m10.filter_extremes(no_below=5, no_above=0.5)   # do not consider all words that occur in less than n=5 documents\n",
    "                                                    # or in more than 50% of all documents.\n",
    "\n",
    "ldacorpus_m10 = [id2word_m10.doc2bow(doc) for doc in ldainput_m10]\n",
    "tfidfcorpus_m10 = models.TfidfModel(ldacorpus_m10)\n",
    "lda_m10 = models.ldamodel.LdaModel(corpus=tfidfcorpus_m10[ldacorpus_m10],id2word=id2word_m10,num_topics=10, passes=5, alpha='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert your topics back to a df\n",
    "all_topics = lda_m10.get_document_topics(ldacorpus_m10, minimum_probability=0.0)\n",
    "all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "\n",
    "all_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide each value in a row by the sum of the row to normalize the values\n",
    "all_topics_df.rename(columns= { 0 : \"topic_0\", 1 : \"topic_1\", 2 : \"topic_2\", 3 : \"topic_3\",\n",
    "4 : \"topic_4\", 5 : \"topic_5\", 6 : \"topic_6\", 7 : \"topic_7\", 8 : \"topic_8\", 9 : \"topic_9\" },inplace=True)\n",
    "all_topics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create a df with the dates and orginal texts\n",
    "\n",
    "meta = pd.DataFrame(zip(dates, speeches_eng))\n",
    "meta.rename(columns = {0: \"date\", 1 : \"speeches\"}, inplace=True)\n",
    "\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge with the topic scores per document\n",
    "\n",
    "final = pd.concat([meta, all_topics_df], axis=1)\n",
    "final['id'] = final.index\n",
    "final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### reshape to long format\n",
    "\n",
    "long = pd.melt(final, id_vars=['id', 'date', 'speeches'], value_vars=[f\"topic_{i}\" for i in range(0,10)])\n",
    "long.rename(columns={\"variable\" : \"topic_nr\", \"value\" : \"topic_score\"}, inplace=True)\n",
    "long.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data in right shape for plotting\n",
    "\n",
    "long['date'] = long['date'].map(pd.to_datetime)\n",
    "plotdf = long.groupby(['topic_nr',pd.Grouper(key='date', freq='Y')]).mean().reset_index()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "sns.lineplot(x=\"date\", y=\"topic_score\", hue=\"topic_nr\", data=plotdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FINISHED RUNNING:\",str(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
