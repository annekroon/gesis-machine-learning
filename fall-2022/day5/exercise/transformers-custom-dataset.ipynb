{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bc832a",
   "metadata": {},
   "source": [
    "# Fine tuning transformers models on a custom dataset in a down-stream classification task\n",
    "\n",
    "Today, we will return to the dataset that we've used on day 1 of our course: The ImDB data. Go back to the code you've written, and inspect the `recall`, `precision`, and `f1-scores`. \n",
    "\n",
    "In this notebook, we will try to improve the performance of our classifier by using `transfer learning`. In this notebook, we will use a `BERT` model, but feel free to check out the HuggingFace liberary whether there are alternatives that you might want to use. \n",
    "\n",
    "\n",
    "If your system does not run on GPU's, it is adviced to run this Notebook in Colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe50f",
   "metadata": {},
   "source": [
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/annekroon/gesis-machine-learning/blob/main/day5/imdb.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c986d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gdown\n",
    "!pip3 install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb18958",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gdown\n",
    "import gzip\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import ticker\n",
    "sns.set(style='ticks', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f2b4c",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6092b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL = 'distilbert-base-cased'  #Insert here the name of the model that you want to work with. You can inspect different models at huggingface: https://huggingface.co/models\n",
    "DEVICE = 'cuda'       \n",
    "MAX_LENGTH = 512   # This is the maximum token length                                                  \n",
    "CACHED_DIR = 'my-awesome-model'  # directory that we'll use for saving the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a7a26",
   "metadata": {},
   "source": [
    "### Read IMBD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir is \"neg\" else 1)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfb0ac",
   "metadata": {},
   "source": [
    "Create train and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('aclImdb/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145bc1a",
   "metadata": {},
   "source": [
    "Split train samples in train and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48aa0d6",
   "metadata": {},
   "source": [
    "### Run a simply traditional classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbe0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000).fit(X_train, train_labels)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d401ad",
   "metadata": {},
   "source": [
    "### Let's start with our transformer-based approach\n",
    "\n",
    "First, we need to tokenize the data using a tokenizer provided by HuggingFace. In particular, you need a tokenizer that belongs to the particular language model you will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e885f",
   "metadata": {},
   "source": [
    "tokenize the train/ val and test datasets, apply pedding and truncation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84bc841",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LENGTH)\n",
    "test_encodings  = tokenizer(test_texts, truncation=True, padding=True, max_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338140b",
   "metadata": {},
   "source": [
    "### Use the `PyTorch` Dataset class to transform the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057f6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7b3b3a",
   "metadata": {},
   "source": [
    "### Inspect the results of the tokenization proces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558bad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(train_encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651529fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(test_encodings[0].tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68fff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(train_dataset.encodings[0].tokens[0:100])\n",
    "' '.join(test_dataset.encodings[1].tokens[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee718f",
   "metadata": {},
   "source": [
    "### You can custimize the evaluation metrics that the model will provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    return {\n",
    "      'accuracy': acc, \n",
    "      'f1' : f1,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ad2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a ForSequenceClassification model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL, num_labels=len(id2label)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508d938",
   "metadata": {},
   "source": [
    "### If needed, tweak the `Trainer` class parameter settings, and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b104064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86637a88",
   "metadata": {},
   "source": [
    "### Evaluate the model on the validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_validation = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37efc7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_val_labels = predicted_validation.predictions.argmax(-1) # Get the highest probability prediction\n",
    "predicted_val_labels = predicted_val_labels.flatten().tolist()      # Flatten the predictions into a 1D list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(val_labels, predicted_val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200928a9",
   "metadata": {},
   "source": [
    "### Evaluation on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test = trainer.predict(test_dataset)\n",
    "predicted_test_labels = predicted_test.predictions.argmax(-1) # Get the highest probability prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_test_labels = predicted_test_labels.flatten().tolist()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421aa922",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, predicted_test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
